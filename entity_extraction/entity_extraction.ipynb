{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# based on https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/client-libraries-rest-api?pivots=programming-language-python&tabs=version-3-1#named-entity-recognition-(ner)\n",
    "# requirements:\n",
    "# pip install azure-ai-textanalytics --pre\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('.azure-key') as fh:\n",
    "    key = fh.read()\n",
    "endpoint = 'https://climate-law-entity-extraction.cognitiveservices.azure.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint,\n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def chunks(list, num_elements):\n",
    "    for i in range(0, len(list), num_elements):\n",
    "        yield list[i:i + num_elements]\n",
    "\n",
    "def read_file_in_parts(file_path, part_length_limit):\n",
    "    \"\"\"\n",
    "     splits the file along newlines to parts with given max length\n",
    "    \"\"\"\n",
    "    document_parts = []\n",
    "    with open(file_path, mode='r', encoding='utf8') as input_document:\n",
    "            lines = input_document.readlines()\n",
    "            part_character_count = 0\n",
    "            part = ''\n",
    "            for line in lines:\n",
    "                line_length = len(line.encode('utf8'))\n",
    "                if part_character_count + line_length > part_length_limit:\n",
    "                    document_parts.append(part)\n",
    "                    part = ''\n",
    "                    part_character_count = 0\n",
    "\n",
    "                if line_length > part_length_limit:\n",
    "                    print(\"ERROR: line too long: \" + line)\n",
    "                    continue\n",
    "\n",
    "                part += line + '\\n'\n",
    "                part_character_count += line_length\n",
    "    return document_parts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Actual limit of service is 5120 \"text elements\" but having a hard time matching this with characters or bytes in python\n",
    "DOCUMENT_CHARACTER_LIMIT = 4000\n",
    "VERBOSE = False\n",
    "\n",
    "def entity_recognition_from_file(client, file_path, output_dir):\n",
    "    \"\"\"\n",
    "    reads document from file, extracts entities using Azure and writes to output file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        document_parts = read_file_in_parts(file_path, DOCUMENT_CHARACTER_LIMIT)\n",
    "\n",
    "        aggregated_result = []\n",
    "        for chunk in chunks(document_parts, 5):\n",
    "            aggregated_result.extend(client.recognize_entities(documents=chunk))\n",
    "\n",
    "        category_statistics = defaultdict(int)\n",
    "        text_statistics = defaultdict(int)\n",
    "        entities = []\n",
    "        for result in aggregated_result:\n",
    "            if VERBOSE: print(\"Named Entities:\\n\")\n",
    "            for entity in result.entities:\n",
    "                if entity.category == \"Quantity\":\n",
    "                    # We skip quantities for now to reduce some noise, as we probably cant put it to use right now\n",
    "                    continue\n",
    "\n",
    "                category_statistics[entity.category] += 1\n",
    "                subcategory_key = '{}_{}'.format(entity.category, entity.subcategory)\n",
    "                category_statistics[subcategory_key] += 1\n",
    "                text_statistics[entity.text] += 1\n",
    "                entities.append({'text': entity.text, 'category': entity.category, 'subcategory': entity.subcategory, 'confidence_score': entity.confidence_score, 'offset': entity.offset, 'length': entity.length})\n",
    "                if VERBOSE: print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\tLength: \\t\", entity.length, \"\\tOffset: \\t\", entity.offset, \"\\n\")\n",
    "\n",
    "        basename = os.path.basename(file_path)\n",
    "        filename, extension = os.path.splitext(basename)\n",
    "        target_filename = os.path.join(output_dir, filename + '_entities.json')\n",
    "        print(\"Writing {} entities to {}\".format(len(entities), target_filename))\n",
    "\n",
    "        result_dump = {'category_statistics': category_statistics, 'text_statistics': text_statistics, 'entities': entities}\n",
    "        with open(target_filename, mode='w', encoding='utf8') as entities_fh:\n",
    "            entities_fh.write(json.dumps(result_dump))\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run for single document\n",
    "entity_recognition_from_file(client, '../documents/1004_0.txt', '../entities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents_directory = '../documents'\n",
    "entities_directory = '../entities'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# single threaded\n",
    "for entry in os.listdir(documents_directory):\n",
    "    source_file = os.path.join(documents_directory, entry)\n",
    "    if not os.path.isfile(source_file):\n",
    "        continue\n",
    "    entity_recognition_from_file(client, source_file, entities_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "\n",
    "# multi threaded execution\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for entry in os.listdir(documents_directory):\n",
    "        source_file = os.path.join(documents_directory, entry)\n",
    "        if not os.path.isfile(source_file):\n",
    "            continue\n",
    "\n",
    "        future = executor.submit(entity_recognition_from_file, client, source_file, entities_directory)\n",
    "        futures.append(future)\n",
    "\n",
    "    print([f.result() for f in futures])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}